# XrootD Documentation Conversion to MD testing
This repository is used to provide an analysis and data on the performance of various LLMs in converting raw XrootD documentation. It provides you with multiple examples of code, scripts, and converted markdown documents.

> [!IMPORTANT]  
> All testing was done on the second XrootD documentation that was available on the documentation website. A link is available to the original [here](https://xrootd.web.cern.ch/doc/dev55/Syntax_config.htm).
> However, this page was modified to remove the redundant and unnecessary first section that included logos, licensing information, and other details. The modified code can be found [here](ExampleDoc/file.html).

**Guide to repository:**
- `ExamplesDoc` contains all the generated results and the original file.
- `scripts` contains the one Python script so far.
___
## Potential Conversion Solutions: 

**1. Programatically *(Algorithmically)*:** <a name="sol-1"></a>
- Benefits:
  - Zero data loss
  - Extremely fast
- Issues:
  - Poor formatting (inconsistent spacing, text disjointed, and hard to read/interpret)
  - The algorithmic conversion can sometimes lead to artifacts.
- [Example](ExampleDoc/Algorithmic.md)
  - This [markdown](ExampleDoc/Algorithmic.md) was generated by [a script I created](scripts/convert.py) using the [markdownify](https://github.com/matthewwithanm/python-markdownify) python library.

**2. Using LLM's *(AI)*:** <a name="sol-2"></a>
- Benefits:
  - Good formatting (sometimes better looking than the real document)
  - Most likely able to be understood and in a format that LLMs can read
- Issues:
  - Extremely Inconsistent
  - Potentially large data loss
  - The optimal model needs to be chosen
- [Example from ChatGPT](ExampleDoc/CHATGPT/ChatGPT-Only.md)
  - Demonstrates significant data loss
- [Example from Gemma 27B](ExampleDoc/Gemma%203%2027B/gemm_3_27b_it-Only.md)
  - Demonstrates significant accuracy, but lacks major reformatting that deviates from the original document (could be a good thing)
  - Took over **900s** to generate ***ON*** google's servers

**3. Using a combination of LLMs and algorithms:** <a name="sol-3"></a>
- Method: First, running it through an algorithm, then having LLMs reformat its output
- Benefits:
  - By first running it through an algorithm before prompting an LLM with the file, it reduces the effort to convert it. Instead of converting from HTML to Markdown, the LLM is only fixing a given Markdown file that has all the information.
  - Much less data loss when compared to regular LLM conversions (HTML -> Markdown)
  - Much better formatting when compared to the output of an algorithm (almost comparable to the regular LLM output)
  - Created by LLMs, meaning they should be able to understand it
- Issues:
  - Two steps required
  - Data loss still happens
  - The model still needs to be chosen
- [Example](ExampleDoc/Gemini%202.5%20Pro/Gemini_2.5_Pro_Thinking-Only.md) using this [script](scripts/convert.py) in combination with Gemini 2.5 Pro

___
## How this was tested:

First focus was on optimizing a purely algorithmic method [(solution 1)](#sol-1) starting from the [markdownify](https://github.com/matthewwithanm/python-markdownify) Python library. After that, I extended this library and [monkey-patched](https://en.wikipedia.org/wiki/Monkey_patch) some elements to optimize the way it scraped information from the real document. The code for this can be found [here](scripts/convert.py).

Testing this on the test [file](ExampleDoc/file.html) resulted in a decent [result](ExampleDoc/Algorithmic.md) that had formatting issues and large amounts of whitespace between pieces of text (you can see these issues by viewing the source for that markdown file).

After this, multiple different LLMS were tested to varying degrees of accuracy, speed, and issues. To test these LLMs, the prompts were standardized, and each LLM was prompted once for fair results. 

The first prompt ***"HTML -> Markdown"*** [(solution 2)](#sol-2):

> Added file: [file.html](ExampleDoc/file.html)
> 
> Prompt: "Please fully convert this to markdown without using code. Do not skip information:"


The second prompt ***"HTML -> Markdown"*** [(solution 3)](#sol-3):

> Added file: [file.html](ExampleDoc/Algorithmic.md)
> 
> Prompt: "I would like you to fix another version of a markdown file. I have uploaded the file. Please reformat the markdown file to the best of your ability."

___
## Results:

**Solution 1:**

| Algorithm | Result | Notes |
| ------------- | ------------- | ------------- |
| [This Python Script](scripts/convert.py) | [Algorithmic.md](ExampleDoc/Algorithmic.md) | Extremely accurate, almost unreadable in plaintext, and difficult to understand in markdown |

**Solution 2:**

| Model | Result | Accuracy | Formatting | Notes |
| ------------- | ------------- | ------------- | ------------- | ------------- | 
| ChatGPT | [file](ExampleDoc/CHATGPT/ChatGPT-Only.md) | Lost a lot of information and data, especially at the end; Modified some words and grammar. | Extremely good formatting of code | Omitted the entirety of section 5, which was arguably unimportant |
| Gemini 2.0 Flash | [file](ExampleDoc/Gemini%202.0%20Flash/Gemini_2.0_Flash-Only.md) | Most of the information is present | Formatting comparable to algorithmically, confusing and difficult to read/understand | Extremely fast | 
| Gemini 2.5 Flash | [file](ExampleDoc/Gemini%202.5%20Flash/Gemini_2.5_Flash_Thinking-Only.md) | Almost all of the information is present | Bad code formatting; Code formatting includes markdown information; Bad formatting in general |  Thinking Mode |
| Gemini 2.5 Pro | [file](ExampleDoc/Gemini%202.5%20Pro/Gemini_2.5_Pro_Thinking-Only.md) | All the information is present | Good formatting besides table of contents. |  Thinking Mode |
| Gemini 3 27B | [file](ExampleDoc/Gemma%203%2027B/gemma_3_27b_it-Only.md) | **ALL** of the information is present!! (except table of contents) | Extremely accurate formatting. Almost identical to regular. | **Took OVER 900 seconds to generate** |

**Solution 3:**

| Model | Result | Accuracy | Formatting | Notes |
| ------------- | ------------- | ------------- | ------------- | ------------- | 
| ChatGPT | [file](ExampleDoc/CHATGPT/ChatGPT-Reformat.md) | Some information was altered to change grammar or omit unnecessary words. | Extremely good formatting of code (again); Formatting better than the original | - |
| Gemini 2.0 Flash | [file](ExampleDoc/Gemini%202.0%20Flash/Gemini_2.0_Flash-Reformat.md) | All information present | Formatting is bad, weird use of code blocks; confusing, and difficult to read/understand | Extremely fast (again) | 
| Gemini 2.5 Flash | [file](ExampleDoc/Gemini%202.5%20Flash/Gemini_2.5_Flash_Thinking-Reformat.md) | Almost all of the information is present | Bad formatting in general, weird use of code blocks; Code formatting includes markdown information; |  Thinking Mode |
| Gemini 2.5 Pro | [file](ExampleDoc/Gemini%202.5%20Pro/Gemini_2.5_Pro_Thinking-Reformat.md) | All the information is present again | Good formatting besides the table of contents ...again. |  Thinking Mode |
| Gemini 3 27B | [file](ExampleDoc/Gemma%203%2027B/gemma_3_27b_it-Reformat.md) | All of the information is present!! (except table of contents ... again) | Horrible formatting, similar to both flash models. | **Also took OVER 900 seconds to generate ... AGAIN** |

____
Please give your opinions on which you prefer, add your own models, and contribute in any way you see fit!

Adapted from the original [Repo](https://github.com/OakSwingZZZ/mdConversionTesting)


